{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJD3JEpllWIn",
        "outputId": "e0fe9f21-07cf-47ea-bb3e-7d4710484d6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KehgemGblZ4V",
        "outputId": "ebe9ed90-75b7-473f-a2bf-4ccc0834c4dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Thesis_work/DGCNN/dgcnn/pytorch\n"
          ]
        }
      ],
      "source": [
        "%cd /content/gdrive/MyDrive/Thesis_work/DGCNN/dgcnn/pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YW_6JoInlnLR"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from data import ModelNet40\n",
        "from model import PointNet, DGCNN\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from util import cal_loss, IOStream\n",
        "import sklearn.metrics as metrics\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "\n",
        "import sys\n",
        "import copy\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxwWBk5xlrEh",
        "outputId": "cb64079a-c280-40fd-b2e4-dcb0bfe73c42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting path.py\n",
            "  Downloading path.py-12.5.0-py3-none-any.whl (2.3 kB)\n",
            "Collecting path\n",
            "  Downloading path-16.3.0-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: path, path.py\n",
            "Successfully installed path-16.3.0 path.py-12.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install path.py;\n",
        "from path import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1q4cw6qltge",
        "outputId": "6eebae6b-1b85-4310-d20f-4f97f2bbd530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Thesis_work/POINTNET/ModelNet30\n"
          ]
        }
      ],
      "source": [
        "random.seed = 49\n",
        "path = Path(\"/content/gdrive/MyDrive/Thesis_work/POINTNET/ModelNet30\")\n",
        "print(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SABhpyTflyPu",
        "outputId": "a4175e4c-9f27-4712-cbb9-76332f1638f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'airplane': 0,\n",
              " 'bench': 1,\n",
              " 'bookshelf': 2,\n",
              " 'bottle': 3,\n",
              " 'bowl': 4,\n",
              " 'car': 5,\n",
              " 'cone': 6,\n",
              " 'cup': 7,\n",
              " 'curtain': 8,\n",
              " 'door': 9,\n",
              " 'flower_pot': 10,\n",
              " 'glass_box': 11,\n",
              " 'guitar': 12,\n",
              " 'keyboard': 13,\n",
              " 'lamp': 14,\n",
              " 'laptop': 15,\n",
              " 'mantel': 16,\n",
              " 'person': 17,\n",
              " 'piano': 18,\n",
              " 'plant': 19,\n",
              " 'radio': 20,\n",
              " 'range_hood': 21,\n",
              " 'sink': 22,\n",
              " 'stairs': 23,\n",
              " 'stool': 24,\n",
              " 'tent': 25,\n",
              " 'tv_stand': 26,\n",
              " 'vase': 27,\n",
              " 'wardrobe': 28,\n",
              " 'xbox': 29}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "folders = [dir for dir in sorted(os.listdir(path)) if os.path.isdir(path/dir)]\n",
        "classes = {folder: i for i, folder in enumerate(folders)};\n",
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zr2oJpOl1Za"
      },
      "outputs": [],
      "source": [
        "def read_off(file):\n",
        "    first_line = file.readline()\n",
        "    if len(first_line) != 4:\n",
        "        #first_line = first_line.decode(\"utf-8\")\n",
        "        #print(type(first_line))\n",
        "        #print(first_line)\n",
        "        line = first_line.strip('OFF').split()\n",
        "        n_verts, n_faces, __ = tuple([int(s) for s in line])\n",
        "    else:\n",
        "        n_verts, n_faces, __ = tuple([int(s) for s in file.readline().strip().split(' ')])\n",
        "    # if 'OFF' != file.readline().strip():\n",
        "    #     raise('Not a valid OFF header')\n",
        "    \n",
        "    verts = [[float(s) for s in file.readline().strip().split(' ')] for i_vert in range(n_verts)]\n",
        "    faces = [[int(s) for s in file.readline().strip().split(' ')][1:] for i_face in range(n_faces)]\n",
        "    return verts, faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Lpm8WjFl7Ky"
      },
      "outputs": [],
      "source": [
        "class PointSampler(object):\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, int)\n",
        "        self.output_size = output_size\n",
        "    \n",
        "    def triangle_area(self, pt1, pt2, pt3):\n",
        "        side_a = np.linalg.norm(pt1 - pt2)\n",
        "        side_b = np.linalg.norm(pt2 - pt3)\n",
        "        side_c = np.linalg.norm(pt3 - pt1)\n",
        "        s = 0.5 * ( side_a + side_b + side_c)\n",
        "        return max(s * (s - side_a) * (s - side_b) * (s - side_c), 0)**0.5\n",
        "\n",
        "    def sample_point(self, pt1, pt2, pt3):\n",
        "        # barycentric coordinates on a triangle\n",
        "        # https://mathworld.wolfram.com/BarycentricCoordinates.html\n",
        "        s, t = sorted([random.random(), random.random()])\n",
        "        f = lambda i: s * pt1[i] + (t-s)*pt2[i] + (1-t)*pt3[i]\n",
        "        return (f(0), f(1), f(2))\n",
        "        \n",
        "    \n",
        "    def __call__(self, mesh):\n",
        "        verts, faces = mesh\n",
        "        verts = np.array(verts)\n",
        "        areas = np.zeros((len(faces)))\n",
        "\n",
        "        for i in range(len(areas)):\n",
        "            areas[i] = (self.triangle_area(verts[faces[i][0]],\n",
        "                                           verts[faces[i][1]],\n",
        "                                           verts[faces[i][2]]))\n",
        "            \n",
        "        sampled_faces = (random.choices(faces, \n",
        "                                      weights=areas,\n",
        "                                      cum_weights=None,\n",
        "                                      k=self.output_size))\n",
        "        \n",
        "        sampled_points = np.zeros((self.output_size, 3))\n",
        "\n",
        "        for i in range(len(sampled_faces)):\n",
        "            sampled_points[i] = (self.sample_point(verts[sampled_faces[i][0]],\n",
        "                                                   verts[sampled_faces[i][1]],\n",
        "                                                   verts[sampled_faces[i][2]]))\n",
        "        \n",
        "        return sampled_points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQYxi9EQmBEd"
      },
      "outputs": [],
      "source": [
        "class Normalize(object):\n",
        "    def __call__(self, pointcloud):\n",
        "        assert len(pointcloud.shape)==2\n",
        "        \n",
        "        norm_pointcloud = pointcloud - np.mean(pointcloud, axis=0) \n",
        "        norm_pointcloud /= np.max(np.linalg.norm(norm_pointcloud, axis=1))\n",
        "\n",
        "        return  norm_pointcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pny8t0qHmE0-"
      },
      "outputs": [],
      "source": [
        "class RandRotation_z(object):\n",
        "    def __call__(self, pointcloud):\n",
        "        assert len(pointcloud.shape)==2\n",
        "\n",
        "        theta = random.random() * 2. * math.pi\n",
        "        rot_matrix = np.array([[ math.cos(theta), -math.sin(theta),    0],\n",
        "                               [ math.sin(theta),  math.cos(theta),    0],\n",
        "                               [0,                             0,      1]])\n",
        "        \n",
        "        rot_pointcloud = rot_matrix.dot(pointcloud.T).T\n",
        "        return  rot_pointcloud\n",
        "    \n",
        "class RandomNoise(object):\n",
        "    def __call__(self, pointcloud):\n",
        "        assert len(pointcloud.shape)==2\n",
        "\n",
        "        noise = np.random.normal(0, 0.02, (pointcloud.shape))\n",
        "    \n",
        "        noisy_pointcloud = pointcloud + noise\n",
        "        return  noisy_pointcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGkkLuuAmHXG"
      },
      "outputs": [],
      "source": [
        "class ToTensor(object):\n",
        "    def __call__(self, pointcloud):\n",
        "        assert len(pointcloud.shape)==2\n",
        "\n",
        "        return torch.from_numpy(pointcloud)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIE4q6KumJys"
      },
      "outputs": [],
      "source": [
        "def default_transforms():\n",
        "    return transforms.Compose([\n",
        "                                PointSampler(1024),\n",
        "                                Normalize(),\n",
        "                                ToTensor()\n",
        "                              ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zREqxQBAmMat"
      },
      "outputs": [],
      "source": [
        "class PointCloudData(Dataset):\n",
        "    def __init__(self, root_dir, valid=False, folder=\"train\", transform=default_transforms()):\n",
        "        self.root_dir = root_dir\n",
        "        folders = [dir for dir in sorted(os.listdir(root_dir)) if os.path.isdir(root_dir/dir)]\n",
        "        self.classes = {folder: i for i, folder in enumerate(folders)}\n",
        "        self.transforms = transform if not valid else default_transforms()\n",
        "        self.valid = valid\n",
        "        self.files = []\n",
        "        for category in self.classes.keys():\n",
        "            new_dir = root_dir/Path(category)/folder\n",
        "            for file in os.listdir(new_dir):\n",
        "                if file.endswith('.off'):\n",
        "                    sample = {}\n",
        "                    sample['pcd_path'] = new_dir/file\n",
        "                    sample['category'] = category\n",
        "                    self.files.append(sample)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __preproc__(self, file):\n",
        "        verts, faces = read_off(file)\n",
        "        if self.transforms:\n",
        "            pointcloud = self.transforms((verts, faces))\n",
        "        return pointcloud\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pcd_path = self.files[idx]['pcd_path']\n",
        "        category = self.files[idx]['category']\n",
        "        with open(pcd_path, 'r') as f:\n",
        "            pointcloud = self.__preproc__(f)\n",
        "        return {'pointcloud': pointcloud, \n",
        "                'category': self.classes[category]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTT1VShLmSUX"
      },
      "outputs": [],
      "source": [
        "train_transforms = transforms.Compose([\n",
        "                    PointSampler(1024),\n",
        "                    Normalize(),\n",
        "                    RandRotation_z(),\n",
        "                    RandomNoise(),\n",
        "                    ToTensor()\n",
        "                    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWcjG9lqmX5y"
      },
      "outputs": [],
      "source": [
        "train_ds = PointCloudData(path, transform=train_transforms)\n",
        "valid_ds = PointCloudData(path, valid=True, folder='test', transform=train_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuBk_WjVwCKZ",
        "outputId": "3478f0bd-f789-4b32-abe4-30087d35eb31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_w39kZimyqs",
        "outputId": "e9f6bce5-34f1-4d1b-f604-9e155fb57c0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading all train PCD files \n",
            "Samples read: 0-100-200-300-400-500-600-700-800-900-1000-1100-1200-1300-1400-1500-1600-1700-1800-1900-2000-2100-2200-2300-2400-2500-2600-2700-2800-2900-"
          ]
        }
      ],
      "source": [
        "print(\"Reading all train PCD files \\nSamples read:\",end=\" \")\n",
        "X = []\n",
        "for i in range(len(train_ds)):\n",
        "    if i%100==0:\n",
        "        print(i,end=\"-\")\n",
        "    X.append(train_ds[i])   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCzUR2dom9by"
      },
      "outputs": [],
      "source": [
        "print(\"Reading all valid PCD files \\nSamples read:\",end=\" \")\n",
        "X_val = []\n",
        "for i in range(len(valid_ds)):\n",
        "    if i%100==0:\n",
        "        print(i,end=\"-\")\n",
        "    X_val.append(valid_ds[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Gc-sK36nBO0"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(dataset=X, batch_size=32, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(dataset=X_val, batch_size=16, shuffle=False, drop_last=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_hDbPlMnNp-"
      },
      "outputs": [],
      "source": [
        "def knn(x, k):\n",
        "    inner = -2*torch.matmul(x.transpose(2, 1), x)\n",
        "    xx = torch.sum(x**2, dim=1, keepdim=True)\n",
        "    pairwise_distance = -xx - inner - xx.transpose(2, 1)\n",
        " \n",
        "    idx = pairwise_distance.topk(k=k, dim=-1)[1]   # (batch_size, num_points, k)\n",
        "    return idx\n",
        "\n",
        "\n",
        "def get_graph_feature(x, k=20, idx=None):\n",
        "    batch_size = x.size(0)\n",
        "    num_points = x.size(2)\n",
        "    x = x.view(batch_size, -1, num_points)\n",
        "    if idx is None:\n",
        "        idx = knn(x, k=k)   # (batch_size, num_points, k)\n",
        "    device = torch.device('cuda')\n",
        "\n",
        "    idx_base = torch.arange(0, batch_size, device=device).view(-1, 1, 1)*num_points\n",
        "\n",
        "    idx = idx + idx_base\n",
        "\n",
        "    idx = idx.view(-1)\n",
        " \n",
        "    _, num_dims, _ = x.size()\n",
        "\n",
        "    x = x.transpose(2, 1).contiguous()   # (batch_size, num_points, num_dims)  -> (batch_size*num_points, num_dims) #   batch_size * num_points * k + range(0, batch_size*num_points)\n",
        "    feature = x.view(batch_size*num_points, -1)[idx, :]\n",
        "    feature = feature.view(batch_size, num_points, k, num_dims) \n",
        "    x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1)\n",
        "    \n",
        "    feature = torch.cat((feature-x, x), dim=3).permute(0, 3, 1, 2).contiguous()\n",
        "  \n",
        "    return feature\n",
        "\n",
        "class DGCNN(nn.Module):\n",
        "    def __init__(self, output_channels=30):\n",
        "        super(DGCNN, self).__init__()\n",
        "        self.k = 20\n",
        "        \n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.bn5 = nn.BatchNorm1d(1024)\n",
        "\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(6, 64, kernel_size=1, bias=False),\n",
        "                                   self.bn1,\n",
        "                                   nn.LeakyReLU(negative_slope=0.2))\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n",
        "                                   self.bn2,\n",
        "                                   nn.LeakyReLU(negative_slope=0.2))\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(64*2, 128, kernel_size=1, bias=False),\n",
        "                                   self.bn3,\n",
        "                                   nn.LeakyReLU(negative_slope=0.2))\n",
        "        self.conv4 = nn.Sequential(nn.Conv2d(128*2, 256, kernel_size=1, bias=False),\n",
        "                                   self.bn4,\n",
        "                                   nn.LeakyReLU(negative_slope=0.2))\n",
        "        self.conv5 = nn.Sequential(nn.Conv1d(512, 1024, kernel_size=1, bias=False),\n",
        "                                   self.bn5,\n",
        "                                   nn.LeakyReLU(negative_slope=0.2))\n",
        "        self.linear1 = nn.Linear(1024*2, 512, bias=False)\n",
        "        self.bn6 = nn.BatchNorm1d(512)\n",
        "        self.dp1 = nn.Dropout(p=0.6)\n",
        "        self.linear2 = nn.Linear(512, 256)\n",
        "        self.bn7 = nn.BatchNorm1d(256)\n",
        "        self.dp2 = nn.Dropout(p=0.6)\n",
        "        self.linear3 = nn.Linear(256, output_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = get_graph_feature(x, k=self.k)\n",
        "        x = self.conv1(x)\n",
        "        x1 = x.max(dim=-1, keepdim=False)[0]\n",
        "\n",
        "        x = get_graph_feature(x1, k=self.k)\n",
        "        x = self.conv2(x)\n",
        "        x2 = x.max(dim=-1, keepdim=False)[0]\n",
        "\n",
        "        x = get_graph_feature(x2, k=self.k)\n",
        "        x = self.conv3(x)\n",
        "        x3 = x.max(dim=-1, keepdim=False)[0]\n",
        "      \n",
        "        x = get_graph_feature(x3, k=self.k)\n",
        "        x = self.conv4(x)\n",
        "        x4 = x.max(dim=-1, keepdim=False)[0]\n",
        "      \n",
        "        x = torch.cat((x1, x2, x3, x4), dim=1)\n",
        "\n",
        "        x = self.conv5(x)\n",
        "  \n",
        "        x1 = F.adaptive_max_pool1d(x, 1).view(batch_size, -1)\n",
        "        x2 = F.adaptive_avg_pool1d(x, 1).view(batch_size, -1)\n",
        "        x = torch.cat((x1, x2), 1)\n",
        "        \n",
        "        x = F.leaky_relu(self.bn6(self.linear1(x)), negative_slope=0.2)\n",
        "        x = self.dp1(x)\n",
        "        x = F.leaky_relu(self.bn7(self.linear2(x)), negative_slope=0.2)\n",
        "        x = self.dp2(x)\n",
        "        x = self.linear3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfPUyefVxZs3"
      },
      "outputs": [],
      "source": [
        "def train(epochs,model,train_loader,test_loader, sgd, lr, momentum):\n",
        "    \n",
        "    #Try to load models\n",
        "    # if args.model == 'pointnet':\n",
        "    #     model = PointNet(args).to(device)\n",
        "    # elif args.model == 'dgcnn':\n",
        "    #     model = DGCNN(args).to(device)\n",
        "    # else:\n",
        "    #     raise Exception(\"Not implemented\")\n",
        "    # print(str(model))\n",
        "    \n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    \n",
        "    PATH = \"dgcnn.pth\"\n",
        "#     checkpoint = torch.load(PATH)\n",
        "#     model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "    if sgd:\n",
        "        print(\"Use SGD\")\n",
        "        opt = optim.SGD(model.parameters(), lr=lr*100, momentum=momentum, weight_decay=1e-4)\n",
        "    else:\n",
        "        print(\"Use Adam\")\n",
        "        opt = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "\n",
        "    scheduler = CosineAnnealingLR(opt, epochs, eta_min=0.0001)\n",
        "    \n",
        "    criterion = cal_loss\n",
        "\n",
        "    best_test_acc = 0\n",
        "    for epoch in range(epochs):\n",
        "        opt.step()\n",
        "        scheduler.step()\n",
        "        ####################\n",
        "        # Train\n",
        "        ####################\n",
        "        train_loss = 0.0\n",
        "        count = 0.0\n",
        "        model.train()\n",
        "        train_pred = []\n",
        "        train_true = []\n",
        "        for data in train_loader:\n",
        "            label = data['category'].to(device)\n",
        "            data  = data['pointcloud'].to(device).float() \n",
        "            data = data.permute(0, 2, 1)\n",
        "            batch_size = data.size()[0]\n",
        "            opt.zero_grad()\n",
        "            logits = model(data)\n",
        "            loss = criterion(logits, label)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            preds = logits.max(dim=1)[1]\n",
        "            count += batch_size\n",
        "            train_loss += loss.item() * batch_size\n",
        "            train_true.append(label.cpu().numpy())\n",
        "            train_pred.append(preds.detach().cpu().numpy())\n",
        "        train_true = np.concatenate(train_true)\n",
        "        train_pred = np.concatenate(train_pred)\n",
        "        # torch.save(model.state_dict(), \"save_\"+str(epoch)+\".pth\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': opt.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, \"dgcnn_\"+str(epoch)+\".pth\")\n",
        "        outstr = 'Train %d, loss: %.6f, train acc: %.6f, train avg acc: %.6f' % (epoch,\n",
        "                                                                                 train_loss*1.0/count,\n",
        "                                                                                 metrics.accuracy_score(\n",
        "                                                                                     train_true, train_pred),\n",
        "                                                                                 metrics.balanced_accuracy_score(\n",
        "                                                                                     train_true, train_pred))\n",
        "        print(outstr)\n",
        "\n",
        "        ####################\n",
        "        # Test\n",
        "        ####################\n",
        "        test_loss = 0.0\n",
        "        count = 0.0\n",
        "        model.eval()\n",
        "        test_pred = []\n",
        "        test_true = []\n",
        "        for data in test_loader:\n",
        "            label = data['category'].to(device).squeeze()\n",
        "            data= data['pointcloud'].to(device).float() \n",
        "            data = data.permute(0, 2, 1)\n",
        "            batch_size = data.size()[0]\n",
        "            logits = model(data)\n",
        "            loss = criterion(logits, label)\n",
        "            preds = logits.max(dim=1)[1]\n",
        "            count += batch_size\n",
        "            test_loss += loss.item() * batch_size\n",
        "            test_true.append(label.cpu().numpy())\n",
        "            test_pred.append(preds.detach().cpu().numpy())\n",
        "        test_true = np.concatenate(test_true)\n",
        "        test_pred = np.concatenate(test_pred)\n",
        "        test_acc = metrics.accuracy_score(test_true, test_pred)\n",
        "        avg_per_class_acc = metrics.balanced_accuracy_score(test_true, test_pred)\n",
        "        outstr = 'Test %d, loss: %.6f, test acc: %.6f, test avg acc: %.6f' % (epoch,\n",
        "                                                                              test_loss*1.0/count,\n",
        "                                                                              test_acc,\n",
        "                                                                              avg_per_class_acc)\n",
        "        print(outstr)\n",
        "        if test_acc >= best_test_acc:\n",
        "            best_test_acc = test_acc\n",
        "            #torch.save(model.state_dict(), 'checkpoints/%s/models/model.t7' % args.exp_name)\n",
        "                torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                }, \"dgcnn_\"+str(epoch)+\"_\"+str(best_test_acc+\".pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYykN5ZUna7B"
      },
      "outputs": [],
      "source": [
        "def train(epochs,train_loader,test_loader, sgd, lr, momentum):\n",
        "    # train_loader = DataLoader(ModelNet40(partition='train', num_points=args.num_points), num_workers=8,\n",
        "    #                           batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
        "    # test_loader = DataLoader(ModelNet40(partition='test', num_points=args.num_points), num_workers=8,\n",
        "    #                          batch_size=args.test_batch_size, shuffle=True, drop_last=False)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    #Try to load models\n",
        "    # if args.model == 'pointnet':\n",
        "    #     model = PointNet(args).to(device)\n",
        "    # elif args.model == 'dgcnn':\n",
        "    #     model = DGCNN(args).to(device)\n",
        "    # else:\n",
        "    #     raise Exception(\"Not implemented\")\n",
        "    # print(str(model))\n",
        "    model = DGCNN().to(device)\n",
        "    model = nn.DataParallel(model)\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    \n",
        "    PATH = \"/content/gdrive/MyDrive/Thesis_work/DGCNN/dgcnn/pytorch/dgcnn_13_86.pth\"\n",
        "    checkpoint = torch.load(PATH)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "    if sgd:\n",
        "        print(\"Use SGD\")\n",
        "        opt = optim.SGD(model.parameters(), lr=lr*100, momentum=momentum, weight_decay=1e-4)\n",
        "    else:\n",
        "        print(\"Use Adam\")\n",
        "        opt = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "\n",
        "    scheduler = CosineAnnealingLR(opt, epochs, eta_min=0.0001)\n",
        "    \n",
        "    criterion = cal_loss\n",
        "\n",
        "    best_test_acc = 0\n",
        "    for epoch in range(epochs):\n",
        "        opt.step()\n",
        "        scheduler.step()\n",
        "        ####################\n",
        "        # Train\n",
        "        ####################\n",
        "        train_loss = 0.0\n",
        "        count = 0.0\n",
        "        model.train()\n",
        "        train_pred = []\n",
        "        train_true = []\n",
        "        for data in train_loader:\n",
        "            data, label = data['pointcloud'].to(device=device, dtype=torch.float), data['category'].to(device).squeeze()\n",
        "            data = data.permute(0, 2, 1)\n",
        "            batch_size = data.size()[0]\n",
        "            opt.zero_grad()\n",
        "            logits = model(data)\n",
        "            loss = criterion(logits, label)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            preds = logits.max(dim=1)[1]\n",
        "            count += batch_size\n",
        "            train_loss += loss.item() * batch_size\n",
        "            train_true.append(label.cpu().numpy())\n",
        "            train_pred.append(preds.detach().cpu().numpy())\n",
        "        train_true = np.concatenate(train_true)\n",
        "        train_pred = np.concatenate(train_pred)\n",
        "        # torch.save(model.state_dict(), \"save_\"+str(epoch)+\".pth\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': opt.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, \"dgcnn_\"+str(epoch+14)+\".pth\")\n",
        "        outstr = 'Train %d, loss: %.6f, train acc: %.6f, train avg acc: %.6f' % (epoch,\n",
        "                                                                                 train_loss*1.0/count,\n",
        "                                                                                 metrics.accuracy_score(\n",
        "                                                                                     train_true, train_pred),\n",
        "                                                                                 metrics.balanced_accuracy_score(\n",
        "                                                                                     train_true, train_pred))\n",
        "        print(outstr)\n",
        "\n",
        "        ####################\n",
        "        # Test\n",
        "        ####################\n",
        "        test_loss = 0.0\n",
        "        count = 0.0\n",
        "        model.eval()\n",
        "        test_pred = []\n",
        "        test_true = []\n",
        "        for data in test_loader:\n",
        "            data, label = data['pointcloud'].to(device=device, dtype=torch.float), data['category'].to(device).squeeze()\n",
        "            data = data.permute(0, 2, 1)\n",
        "            batch_size = data.size()[0]\n",
        "            logits = model(data)\n",
        "            loss = criterion(logits, label)\n",
        "            preds = logits.max(dim=1)[1]\n",
        "            count += batch_size\n",
        "            test_loss += loss.item() * batch_size\n",
        "            test_true.append(label.cpu().numpy())\n",
        "            test_pred.append(preds.detach().cpu().numpy())\n",
        "        test_true = np.concatenate(test_true)\n",
        "        test_pred = np.concatenate(test_pred)\n",
        "        test_acc = metrics.accuracy_score(test_true, test_pred)\n",
        "        avg_per_class_acc = metrics.balanced_accuracy_score(test_true, test_pred)\n",
        "        outstr = 'Test %d, loss: %.6f, test acc: %.6f, test avg acc: %.6f' % (epoch,\n",
        "                                                                              test_loss*1.0/count,\n",
        "                                                                              test_acc,\n",
        "                                                                              avg_per_class_acc)\n",
        "        print(outstr)\n",
        "        if test_acc >= best_test_acc:\n",
        "            best_test_acc = test_acc\n",
        "            #torch.save(model.state_dict(), 'checkpoints/%s/models/model.t7' % args.exp_name)\n",
        "            \n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                }, \"dgcnn_\"+str(epoch)+\"_\"+str(best_test_acc+\".pth\")\n",
        "            \n",
        "\n",
        "# def test(args, io):\n",
        "#     test_loader = DataLoader(ModelNet40(partition='test', num_points=args.num_points),\n",
        "#                              batch_size=args.test_batch_size, shuffle=True, drop_last=False)\n",
        "\n",
        "#     device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "#     #Try to load models\n",
        "#     model = DGCNN(args).to(device)\n",
        "#     model = nn.DataParallel(model)\n",
        "#     model.load_state_dict(torch.load(args.model_path))\n",
        "#     model = model.eval()\n",
        "#     test_acc = 0.0\n",
        "#     count = 0.0\n",
        "#     test_true = []\n",
        "#     test_pred = []\n",
        "#     for data, label in test_loader:\n",
        "\n",
        "#         data, label = data.to(device), label.to(device).squeeze()\n",
        "#         data = data.permute(0, 2, 1)\n",
        "#         batch_size = data.size()[0]\n",
        "#         logits = model(data)\n",
        "#         preds = logits.max(dim=1)[1]\n",
        "#         test_true.append(label.cpu().numpy())\n",
        "#         test_pred.append(preds.detach().cpu().numpy())\n",
        "#     test_true = np.concatenate(test_true)\n",
        "#     test_pred = np.concatenate(test_pred)\n",
        "#     test_acc = metrics.accuracy_score(test_true, test_pred)\n",
        "#     avg_per_class_acc = metrics.balanced_accuracy_score(test_true, test_pred)\n",
        "#     outstr = 'Test :: test acc: %.6f, test avg acc: %.6f'%(test_acc, avg_per_class_acc)\n",
        "#     io.cprint(outstr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBWMkLgQnev8"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DGCNN().to(device)\n",
        "model = nn.DataParallel(model)\n",
        "\n",
        "train(20, train_loader, test_loader, False, 0.0001, 0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3Odb4Zori-o"
      },
      "source": [
        "# Extracting Feature Vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkhF2SBhswTs"
      },
      "outputs": [],
      "source": [
        "path = "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoOslsJVrp7m"
      },
      "outputs": [],
      "source": [
        "unseen_test_ds = PointCloudData(path, folder='test', transform=train_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aI4-5LxUsmYO"
      },
      "outputs": [],
      "source": [
        "unseen_test_loader = DataLoader(dataset=unseen_test_ds, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gklzr9ELr591"
      },
      "outputs": [],
      "source": [
        "def knn(x, k):\n",
        "    inner = -2*torch.matmul(x.transpose(2, 1), x)\n",
        "    xx = torch.sum(x**2, dim=1, keepdim=True)\n",
        "    pairwise_distance = -xx - inner - xx.transpose(2, 1)\n",
        " \n",
        "    idx = pairwise_distance.topk(k=k, dim=-1)[1]   # (batch_size, num_points, k)\n",
        "    return idx\n",
        "\n",
        "\n",
        "def get_graph_feature(x, k=20, idx=None):\n",
        "    batch_size = x.size(0)\n",
        "    num_points = x.size(2)\n",
        "    x = x.view(batch_size, -1, num_points)\n",
        "    if idx is None:\n",
        "        idx = knn(x, k=k)   # (batch_size, num_points, k)\n",
        "    device = torch.device('cuda')\n",
        "\n",
        "    idx_base = torch.arange(0, batch_size, device=device).view(-1, 1, 1)*num_points\n",
        "\n",
        "    idx = idx + idx_base\n",
        "\n",
        "    idx = idx.view(-1)\n",
        " \n",
        "    _, num_dims, _ = x.size()\n",
        "\n",
        "    x = x.transpose(2, 1).contiguous()   # (batch_size, num_points, num_dims)  -> (batch_size*num_points, num_dims) #   batch_size * num_points * k + range(0, batch_size*num_points)\n",
        "    feature = x.view(batch_size*num_points, -1)[idx, :]\n",
        "    feature = feature.view(batch_size, num_points, k, num_dims) \n",
        "    x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1)\n",
        "    \n",
        "    feature = torch.cat((feature-x, x), dim=3).permute(0, 3, 1, 2).contiguous()\n",
        "  \n",
        "    return feature\n",
        "\n",
        "class DGCNN(nn.Module):\n",
        "    def __init__(self, output_channels=30):\n",
        "        super(DGCNN, self).__init__()\n",
        "        self.k = 20\n",
        "        \n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.bn5 = nn.BatchNorm1d(1024)\n",
        "\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(6, 64, kernel_size=1, bias=False),\n",
        "                                   self.bn1,\n",
        "                                   nn.LeakyReLU(negative_slope=0.2))\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n",
        "                                   self.bn2,\n",
        "                                   nn.LeakyReLU(negative_slope=0.2))\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(64*2, 128, kernel_size=1, bias=False),\n",
        "                                   self.bn3,\n",
        "                                   nn.LeakyReLU(negative_slope=0.2))\n",
        "        self.conv4 = nn.Sequential(nn.Conv2d(128*2, 256, kernel_size=1, bias=False),\n",
        "                                   self.bn4,\n",
        "                                   nn.LeakyReLU(negative_slope=0.2))\n",
        "        self.conv5 = nn.Sequential(nn.Conv1d(512, 1024, kernel_size=1, bias=False),\n",
        "                                   self.bn5,\n",
        "                                   nn.LeakyReLU(negative_slope=0.2))\n",
        "        self.linear1 = nn.Linear(1024*2, 512, bias=False)\n",
        "        self.bn6 = nn.BatchNorm1d(512)\n",
        "        self.dp1 = nn.Dropout(p=0.6)\n",
        "        self.linear2 = nn.Linear(512, 256)\n",
        "        self.bn7 = nn.BatchNorm1d(256)\n",
        "        self.dp2 = nn.Dropout(p=0.6)\n",
        "        self.linear3 = nn.Linear(256, output_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = get_graph_feature(x, k=self.k)\n",
        "        x = self.conv1(x)\n",
        "        x1 = x.max(dim=-1, keepdim=False)[0]\n",
        "\n",
        "        x = get_graph_feature(x1, k=self.k)\n",
        "        x = self.conv2(x)\n",
        "        x2 = x.max(dim=-1, keepdim=False)[0]\n",
        "\n",
        "        x = get_graph_feature(x2, k=self.k)\n",
        "        x = self.conv3(x)\n",
        "        x3 = x.max(dim=-1, keepdim=False)[0]\n",
        "      \n",
        "        x = get_graph_feature(x3, k=self.k)\n",
        "        x = self.conv4(x)\n",
        "        x4 = x.max(dim=-1, keepdim=False)[0]\n",
        "      \n",
        "        x = torch.cat((x1, x2, x3, x4), dim=1)\n",
        "\n",
        "        x = self.conv5(x)\n",
        "        \n",
        "        x1 = F.adaptive_max_pool1d(x, 1).view(batch_size, -1)\n",
        "        x2 = F.adaptive_avg_pool1d(x, 1).view(batch_size, -1)\n",
        "        x = torch.cat((x1, x2), 1)\n",
        "        y = x\n",
        "        \n",
        "        x = F.leaky_relu(self.bn6(self.linear1(x)), negative_slope=0.2)\n",
        "        x = self.dp1(x)\n",
        "        x = F.leaky_relu(self.bn7(self.linear2(x)), negative_slope=0.2)\n",
        "        x = self.dp2(x)\n",
        "        x = self.linear3(x)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePg1XiEor9LP"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-_qrnzSr963"
      },
      "outputs": [],
      "source": [
        "model = DGCNN().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DykT-tZJsEts"
      },
      "outputs": [],
      "source": [
        "model = nn.DataParallel(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlwIDyVYsQYX"
      },
      "outputs": [],
      "source": [
        "PATH = \"/content/gdrive/MyDrive/Thesis_work/DGCNN/dgcnn/pytorch/dgcnn_13_86.pth\"\n",
        "checkpoint = torch.load(PATH)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9kbqUs0sVpu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RetVOFORsZWa"
      },
      "outputs": [],
      "source": [
        "def feedforward(train_loader):\n",
        "    model.eval()\n",
        "    df = pd.DataFrame(columns=[i for i in range(2049)])\n",
        "    #feature_vector = torch.zeros([1, 1024], dtype=torch.int32).to(device) \n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        data, labels = data['pointcloud'].to(device=device, dtype=torch.float), data['category'].to(device).squeeze()\n",
        "        data = data.permute(0, 2, 1)\n",
        "        batch_size = data.size()[0]\n",
        "        output, feature_vector = model(data)\n",
        "        # inputs, labels = data['pointcloud'].to(device).float(), data['category'].to(device)\n",
        "        # feature_vector, outputs = model(inputs.transpose(1,2))\n",
        "        feature_vector_np = feature_vector.cpu().detach().numpy()\n",
        "        label_np = labels.cpu().detach().numpy()\n",
        "        # print(len(label_np), feature_vector_np.shape)\n",
        "        df.loc[i] = np.append(label_np, feature_vector_np.flatten())\n",
        "        #feature_vector = torch.cat((feature_vector, vector), dim=0)\n",
        "        if i % 100 == 0:\n",
        "            print(f\"DataFrame Size {df.shape}\")\n",
        "            print(df.head(i))\n",
        "    df.to_csv('feature_vector_test_model30.csv')\n",
        "    print(\"Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSh4TMehsaiK"
      },
      "outputs": [],
      "source": [
        "feedforward(train_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DGCNN_new_modelnet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}